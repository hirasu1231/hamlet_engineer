(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{450:function(t,s,a){t.exports=a.p+"assets/img/zoom.9e568f4d.jpg"},451:function(t,s,a){t.exports=a.p+"assets/img/ESPNet_demo.29711d09.png"},452:function(t,s,a){t.exports=a.p+"assets/img/ESPNet_sample3_org.c0f63aa8.jpg"},453:function(t,s,a){t.exports=a.p+"assets/img/ESPNet_sample3_city.f645b4d7.png"},454:function(t,s,a){t.exports=a.p+"assets/img/ESPNet_sample3_pascal.1e7efbe2.png"},494:function(t,s,a){"use strict";a.r(s);var n=a(2),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h2",{attrs:{id:"python-espnetでcityscapesデータセットでsemantic-segmentationの学習を実施する"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#python-espnetでcityscapesデータセットでsemantic-segmentationの学習を実施する"}},[t._v("#")]),t._v(" Python, ESPNetでCityscapesデータセットでSemantic Segmentationの学習を実施する")]),t._v(" "),n("p",[t._v("Semantic Segmentationの中で軽いモデルであるESPNetv2を実装します．本稿ではまず，デモの起動と公開データセットのCityscapesでの学習を実施します．"),n("br"),t._v("\n今回はGoogle ColabとGoogle Driveを連携させて，notebook形式で実行してます．"),n("br")]),t._v(" "),n("blockquote",[n("p",[t._v("Google Colaboratory（以下Google Colab）は、Google社が無料で提供している機械学習の教育や研究用の開発環境です。開発環境はJupyter Notebookに似たインターフェースを持ち、Pythonの主要なライブラリがプリインストールされています。"),n("br"),t._v("\n引用元："),n("a",{attrs:{href:"https://interface.cqpub.co.jp/ail01/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Google Colabの使い方"),n("OutboundLink")],1)])]),t._v(" "),n("p",[t._v("最終的に，人以外の背景を着色して，zoomのバーチャル背景機能のようなクロマキー合成を実装したいです．"),n("br"),t._v(" "),n("img",{attrs:{src:a(450),alt:""}})]),t._v(" "),n("h2",{attrs:{id:"ファイル構成"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ファイル構成"}},[t._v("#")]),t._v(" ファイル構成")]),t._v(" "),n("p",[t._v("プロジェクトディレクトリはsegmentationとしています．度々，省略しています．")]),t._v(" "),n("div",{staticClass:"language-init extra-class"},[n("pre",{pre:!0,attrs:{class:"language-text"}},[n("code",[t._v("segmentation\n├── /EdgeNets\n│   ├── /data_loader\n│   │   └── /segmentation\n│   │       ├── /scripts\n│   │       │   ├── download_cityscapes.sh\n│   │       │   └── (省略)\n│   │       └── /cityscape_script\n│   │           ├── process_cityscapes.py <- コピー元\n│   │           ├── generate_mappings.py <- コピー元\n│   │           └── (省略)\n│   ├── /sample_images <- サンプル画像\n│   ├── /results_segmentation <- モデルの出力ディレクトリ\n│   ├── /result_images <- 着色画像の出力ディレクトリ\n│   │\n│   ├── segmentation_demo.py\n│   ├── train_segmentation.py\n│   ├── test_segmentation.py\n│   ├── process_cityscapes.py <- コピー先\n│   ├── generate_mappings.py <- コピー先\n│   ├── custom_test_segmentation.py <- 新規作成\n│   └── (省略)\n└── ESPNetv2.ipynb <- 実行用ノートブック\n")])])]),n("h2",{attrs:{id:"edgenets-espnetv2"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#edgenets-espnetv2"}},[t._v("#")]),t._v(" EdgeNets(ESPNetv2)")]),t._v(" "),n("h3",{attrs:{id:"edgenetsのダウンロード"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#edgenetsのダウンロード"}},[t._v("#")]),t._v(" EdgeNetsのダウンロード")]),t._v(" "),n("p",[t._v("Google ColabとGoogle Driveを連携させて，gitからESPNetv2を内包している"),n("a",{attrs:{href:"https://github.com/sacmehta/EdgeNets",target:"_blank",rel:"noopener noreferrer"}},[t._v("EdgeNets"),n("OutboundLink")],1),t._v("をダウンロードします．"),n("br"),t._v("\nsegmentationの解説は"),n("a",{attrs:{href:"(https://github.com/sacmehta/EdgeNets/blob/master/README_Segmentation.md)"}},[t._v("README_Segmentation.md")]),t._v("に記載されています．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Google ColabとGoogle Driveを連携")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" google"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("colab "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" drive\ndrive"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mount"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/content/drive'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ディレクトリの移動")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("cd "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("content"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("drive"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("My Drive"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# gitのダウンロード")]),t._v("\n!git clone https"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("github"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("sacmehta"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("EdgeNets"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("git\n")])])]),n("h3",{attrs:{id:"edgenetsの起動チェック"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#edgenetsの起動チェック"}},[t._v("#")]),t._v(" EdgeNetsの起動チェック")]),t._v(" "),n("p",[t._v("正常に起動するか，デフォルトで入っているデモコードでチェックします．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ディレクトリの移動")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("cd EdgeNets\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# デモコード")]),t._v("\npython segmentation_demo"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n")])])]),n("p",[t._v("上記のコードを実行すると，/segmentation_resultsに着色された画像が出力されます．"),n("br"),t._v(" "),n("img",{attrs:{src:a(451),alt:""}})]),t._v(" "),n("h2",{attrs:{id:"cityscapesデータセットで学習デモ"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#cityscapesデータセットで学習デモ"}},[t._v("#")]),t._v(" Cityscapesデータセットで学習デモ")]),t._v(" "),n("p",[t._v("本稿ではまず，Cityscapesという街中の景色のデータセットでespnetv2の学習を実施します．")]),t._v(" "),n("h3",{attrs:{id:"cityscapesデータセットのダウンロード"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#cityscapesデータセットのダウンロード"}},[t._v("#")]),t._v(" Cityscapesデータセットのダウンロード")]),t._v(" "),n("p",[t._v("Cityscapesデータセットをダウンロードするには，まず"),n("a",{attrs:{href:"https://www.cityscapes-dataset.com",target:"_blank",rel:"noopener noreferrer"}},[t._v("Cityscapesのサイト"),n("OutboundLink")],1),t._v("に登録する必要があります．"),n("br"),t._v("\nそして，./EdgeNets/data_loader/segmentation/scripts/download_cityscapes.shに登録したアドレスとパスワードを記入します．")]),t._v(" "),n("div",{staticClass:"language-sh extra-class"},[n("pre",{pre:!0,attrs:{class:"language-sh"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# download_cityscapes.sh")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 8~10行目")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# enter user details")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("uname")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'登録したアドレス'")]),t._v(" \n"),n("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("pass")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'登録したパスワード'")]),t._v("\n")])])]),n("p",[t._v("以下のコマンドを実行し， Cityscapesデータセットをダウンロードします．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 現在ディレクトリ：/content/drive/My\\ Drive/segmentation/EdgeNets")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ディレクトリの移動")]),t._v("\ncd  "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("data_loader"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("scripts \n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ダウンロード実行")]),t._v("\nsh download_cityscapes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sh\n")])])]),n("h3",{attrs:{id:"cityscapesデータセットを学習用にマスキング"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#cityscapesデータセットを学習用にマスキング"}},[t._v("#")]),t._v(" Cityscapesデータセットを学習用にマスキング")]),t._v(" "),n("p",[t._v("学習のためにCityscapesセグメンテーションマスクで処理する必要があります．"),n("br"),t._v(" "),n("a",{attrs:{href:"https://github.com/sacmehta/EdgeNets/blob/master/README_Segmentation.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("sacmehta/EdgeNets/README_Segmentation.md"),n("OutboundLink")],1),t._v("に記述されているコマンドをそのまま実行すると，エラーが発生しますので「process_cityscapes.pyとgenerate_mappings.py」を./EdgeNets直下に置きます．(本稿のエラー集_エラー1参照)")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 現在ディレクトリ：/content/drive/My\\ Drive/segmentation/EdgeNets")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# cp <移動前> <移動先>")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# process_cityscapes.pyを/EdgeNets直下にコピー")]),t._v("\ncp data_loader"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("cityscape_scripts"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("process_cityscapes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py process_cityscapes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# process_cityscapes.pyを/EdgeNets直下にコピー")]),t._v("\ncp data_loader"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("cityscape_scripts"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("generate_mappings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py generate_mappings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n")])])]),n("p",[t._v("「process_cityscapes.pyとgenerate_mappings.py」の場所を変えたので，それに応じて読み込む場所の記述も書き換えます．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# process_cityscapes.py")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 486行目")]),t._v("\ncityscapes_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./vision_datasets/cityscapes/'")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# generate_mappings.py")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 70行目")]),t._v("\ncityscapes_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./vision_datasets/cityscapes/'")]),t._v("\n")])])]),n("p",[t._v("以下のコマンドでマスキングを実行します．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 現在ディレクトリ：/content/drive/My\\ Drive/segmentation/EdgeNets")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# セグメンテーションマスキング")]),t._v("\npython process_cityscapes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\npython generate_mappings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n")])])]),n("h3",{attrs:{id:"学習の第1段階"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#学習の第1段階"}},[t._v("#")]),t._v(" 学習の第1段階")]),t._v(" "),n("p",[t._v("最初の段階では、低解像度の画像を入力として使用して，より大きなバッチサイズに合わせることができます．"),n("br"),t._v("\n学習したモデルは/results_segmentation内に格納されます．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 現在ディレクトリ：/content/drive/My\\ Drive/segmentation/EdgeNets")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Cityscapes dataset")]),t._v("\n!CUDA_VISIBLE_DEVICES"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" python train_segmentation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("model espnetv2 \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("s "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset city \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("path "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("vision_datasets"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("cityscapes"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("batch"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("size "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("crop"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("size "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("lr "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.009")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("scheduler hybrid \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("clr"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("61")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("epochs "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),t._v("\n")])])]),n("h3",{attrs:{id:"学習の第2段階"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#学習の第2段階"}},[t._v("#")]),t._v(" 学習の第2段階")]),t._v(" "),n("p",[t._v("第2段階では、バッチ正規化レイヤーをフリーズしてから，わずかに高い画像解像度で微調整します．"),n("br"),t._v("\n学習したモデルは/results_segmentation内に格納されます．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 現在ディレクトリ：/content/drive/My\\ Drive/segmentation/EdgeNets")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Cityscapes dataset")]),t._v("\nCUDA_VISIBLE_DEVICES"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" python train_segmentation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("model espnetv2\\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("s "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset city \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("path "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("vision_datasets"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("cityscapes"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("batch"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("size "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("crop"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("size "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("lr "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.005")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("scheduler hybrid \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("clr"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("61")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("epochs "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("freeze"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("bn \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("finetune results_segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("model_espnetv2_city"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("s_2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("0_sch_hybrid_loss_ce_res_512_sc_0"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("25_0"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("espnetv2_2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("0_512_best"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pth\n")])])]),n("h2",{attrs:{id:"cityscapesデータセットでテスト"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#cityscapesデータセットでテスト"}},[t._v("#")]),t._v(" Cityscapesデータセットでテスト")]),t._v(" "),n("p",[t._v("/sample_imagesの画像でテストを実施します．"),n("br")]),t._v(" "),n("h3",{attrs:{id:"test-segmentation-pyの改良"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#test-segmentation-pyの改良"}},[t._v("#")]),t._v(" test_segmentation.pyの改良")]),t._v(" "),n("p",[t._v("そこで任意のディレクトリでsegmentationできるように"),n("code",[t._v("test_segmentation.py")]),t._v("を改良して，"),n("code",[t._v("custom_test_segmentation.py")]),t._v("を作成します．"),n("br")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ./EdgeNets/model/segmentation/espnetv2.py")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 36行目付近")]),t._v("\ndec_feat_dict"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'pascal'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'city'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'coco'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'custom'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[n("code",[t._v("custom_test_segmentation.py")]),t._v("では元コードから以下のように若干いじっています．"),n("br")]),t._v(" "),n("ul",[n("li",[t._v("入力画像の読み込みを，「.png・.jpg・.jpeg」に対応させた．")]),t._v(" "),n("li",[t._v("入力画像の読み込みを任意のディレクトリに指定できるようにした．")]),t._v(" "),n("li",[t._v("出力画像を「背景黒の着色画像」から「元画像と直色画像の合成」に変更した．")]),t._v(" "),n("li",[t._v("出力画像の着色の仕方を固定のカラーマップからクラス数で変更できるようにした．")]),t._v(" "),n("li",[t._v("出力画像の保存先を./result_images/<任意のディレクトリ名>に変更した．")]),t._v(" "),n("li",[t._v("コマンドラインの引数を変更\n"),n("ul",[n("li",[t._v("dataset:：任意のデータでsegmentationできるように「custom」を追加")]),t._v(" "),n("li",[t._v("split ：任意のデータでsegmentationできるように「custom」を追加．，")]),t._v(" "),n("li",[t._v("savedir-name：任意の任意のディレクトリ名で保存できるようにしました")])])])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# custom_test_segmentation.py")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" glob\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" argparse "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ArgumentParser\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" PIL "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Image\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" torchvision"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transforms "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" functional "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" F\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tqdm "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tqdm\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" utilities"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("print_utils "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" transforms"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classification"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data_transforms "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" MEAN"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" STD\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" utilities"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" model_parameters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" compute_flops\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ===========================================")]),t._v("\n__author__ "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Sachin Mehta"')]),t._v("\n__license__ "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"MIT"')]),t._v("\n__maintainer__ "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Sachin Mehta"')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ============================================")]),t._v("\n\nIMAGE_EXTENSIONS "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.jpg'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.png'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.jpeg'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("data_transform")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    img "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Image"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("BILINEAR"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    img "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_tensor"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# convert to tensor (values between 0 and 1)")]),t._v("\n    img "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("normalize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" MEAN"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" STD"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# normalize the tensor")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" img\n\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("evaluate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" device"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    im_size "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("tuple")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# get color map for dataset")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" utilities"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("color_map "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VOCColormap\n    cmap "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VOCColormap"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_classes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_color_map_voc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cmap"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n\n    model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("eval")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" imgName "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" tqdm"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        img "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Image"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("imgName"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("convert"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'RGB'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        img_clone "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("copy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        w"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" h "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size\n\n        img "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_transform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        img "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unsqueeze"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# add a batch dimension")]),t._v("\n        img "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        img_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        img_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# remove the batch dimension")]),t._v("\n        img_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("byte"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# get the label map")]),t._v("\n        img_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cpu'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("numpy"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        img_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Image"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fromarray"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# resize to original size")]),t._v("\n        img_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resize"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("w"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" h"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Image"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("NEAREST"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        \n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pascal dataset accepts colored segmentations")]),t._v("\n        img_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("putpalette"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cmap"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# クラスごとに着色")]),t._v("\n        img_out "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" img_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("convert"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'RGB'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        blended "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Image"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("blend"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_clone"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_out"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" alpha"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.7")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# save the segmentation mask")]),t._v("\n        name "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" imgName"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        img_extn "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" imgName"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        name "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{}/{}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("savedir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" name"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("replace"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_extn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'png'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        blended"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("name"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("main")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# read all the images in the folder")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'custom'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            image_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" extn "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" IMAGE_EXTENSIONS"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                image_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"rgb"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'*'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" extn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                image_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" image_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("  glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            seg_classes "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_classes\n            \n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'custom'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            image_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" extn "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" IMAGE_EXTENSIONS"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                image_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'*'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" extn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                image_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" image_list "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("  glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("glob"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            seg_classes "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_classes\n            \n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            print_error_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{} split not yet supported'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            \n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        print_error_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{} dataset not yet supported'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        print_error_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'No files in directory: {}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    print_info_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'# of images for testing: {}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'espnetv2'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("espnetv2 "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" espnetv2_seg\n        args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("classes "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" seg_classes\n        model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" espnetv2_seg"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'dicenet'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dicenet "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" dicenet_seg\n        model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dicenet_seg"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" classes"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("seg_classes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        print_error_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{} network not yet supported'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        exit"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# mdoel information")]),t._v("\n    num_params "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model_parameters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    flops "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" compute_flops"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("input")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Tensor"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    print_info_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'FLOPs for an input of size {}x{}: {:.2f} million'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" flops"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    print_info_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'# of parameters: {}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_params"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weights_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        print_info_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Loading model weights'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        weight_dict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weights_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" map_location"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cpu'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_state_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weight_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        print_info_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Weight loaded successfully'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        print_error_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'weight file does not exist or not specified. Please check: {}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weights_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    num_gpus "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cuda"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("device_count"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    device "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cuda'")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" num_gpus "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'cpu'")]),t._v("\n    model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("device"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("device"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    evaluate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_list"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" device"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("device"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'__main__'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" commons"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("general_details "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" segmentation_models"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" segmentation_datasets\n\n    parser "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ArgumentParser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# mdoel details")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--model'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"espnetv2"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" choices"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("segmentation_models"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Model name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--weights-test'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Pretrained weights directory.'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--s'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'scale'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# dataset details")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--data-path'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('""')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Data directory'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--dataset'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'custom'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" choices"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'custom'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Dataset name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# input details")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--im-size'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nargs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"+"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Image size for testing (W x H)'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--split'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" choices"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'custom'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'data split'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--model-width'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Model width'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--model-height'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Model height'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--channels'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Input channels'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--num-classes'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                        "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ImageNet classes. Required for loading the base network'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_argument"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'--savedir-name'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" default"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'demo'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                        "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("help")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Save folder location'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    args "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" parser"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parse_args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weights_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weight_locations"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("segmentation "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" model_weight_map\n\n        model_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{}_{}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        dataset_key "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{}_{}x{}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("im_size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" model_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" model_weight_map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{} does not exist'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("assert")]),t._v(" dataset_key "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" model_weight_map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("model_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{} does not exist'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dataset_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weights_test "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model_weight_map"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("model_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dataset_key"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'weights'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isfile"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weights_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            print_error_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'weight file does not exist: {}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weights_test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# set-up results path")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'custom'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'train'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'val'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'test'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("savedir "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'results_images/{}_{}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("elif")]),t._v(" args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'custom'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("savedir "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'results_images/{}'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("savedir_name"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            print_error_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{} split not yet supported'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            \n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        print_error_message"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'{} dataset not yet supported'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataset"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isdir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("savedir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("makedirs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("savedir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# This key is used to load the ImageNet weights while training. So, set to empty to avoid errors")]),t._v("\n    args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("weights "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v("\n\n    main"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("args"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"テストの実行"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#テストの実行"}},[t._v("#")]),t._v(" テストの実行")]),t._v(" "),n("p",[t._v("以下のコマンドでテストを実行します．"),n("br"),t._v("\n以下のコマンドでは，pascalとcityscapesの公開学習済みモデルを使っていますが，"),n("code",[t._v("--weights-test")]),t._v("から任意のモデルを指定することができます．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 現在ディレクトリ：/content/drive/My\\ Drive/segmentation/EdgeNets")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Cityscapes molel")]),t._v("\nCUDA_VISIBLE_DEVICES"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" python custom_test_segmentation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("model espnetv2 \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("s "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset custom \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("path "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("sample_images"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("split custom \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("im"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("size "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("num"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("classes "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v(" \\\n                                 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("weights"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("test model"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("model_zoo"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("espnetv2"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("espnetv2_s_2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("0_city_1024x512"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pth \\\n                                 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("savedir"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("name sample_images_city\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Pascal model")]),t._v("\nCUDA_VISIBLE_DEVICES"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" python custom_test_segmentation"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("model espnetv2 \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("s "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset custom \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("path "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("sample_images"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("split custom \\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("im"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("size "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("384")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("384")]),t._v("\\\n                                "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("num"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("classes "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("21")]),t._v(" \\\n                                 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("weights"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("test model"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("model_zoo"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("espnetv2"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("espnetv2_s_2"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("0_pascal_384x384"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pth \\\n                                 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("savedir"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("name sample_images_pascal\n")])])]),n("h3",{attrs:{id:"テストの結果"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#テストの結果"}},[t._v("#")]),t._v(" テストの結果")]),t._v(" "),n("p",[t._v("以下がpascalとcityscapes，それぞれの学習済みモデルで直色した画像です．"),n("br"),t._v("\n人単体でやるならば，pascalの方がいいようです．"),n("br"),t._v("\nオリジナル画像"),n("br"),t._v(" "),n("img",{attrs:{src:a(452),alt:""}})]),t._v(" "),n("p",[t._v("cityscapesモデルの着色画像"),n("br"),t._v(" "),n("img",{attrs:{src:a(453),alt:""}})]),t._v(" "),n("p",[t._v("pascalモデルの着色画像"),n("br"),t._v(" "),n("img",{attrs:{src:a(454),alt:""}})]),t._v(" "),n("h2",{attrs:{id:"まとめ"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#まとめ"}},[t._v("#")]),t._v(" まとめ")]),t._v(" "),n("p",[t._v("本稿ではSemantic Segmentationの中で軽いモデルであるESPNetv2の，デモの起動と公開データセットのCityscapesでの学習を実施しました．"),n("br"),t._v("\nデフォルトのコードでは若干不便なところもあったので，逐次改造したコードを作成しています．"),n("br"),t._v("\n次回からはオリジナルデータで学習することも想定しつつ，人だけを着色するモデルの学習を実施します．")]),t._v(" "),n("h2",{attrs:{id:"参考サイト"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#参考サイト"}},[t._v("#")]),t._v(" 参考サイト")]),t._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/sacmehta/EdgeNets",target:"_blank",rel:"noopener noreferrer"}},[t._v("sacmehta/EdgeNets"),n("OutboundLink")],1),n("br"),t._v(" "),n("a",{attrs:{href:"https://github.com/sacmehta/EdgeNets/blob/master/README_Segmentation.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("sacmehta/EdgeNets/README_Segmentation.md"),n("OutboundLink")],1),t._v(" "),n("a",{attrs:{href:"https://qiita.com/tokyokuma/items/37b1370ea7c84399fbb9",target:"_blank",rel:"noopener noreferrer"}},[t._v("ESPNetで自作データセットを学習してセグメンテーション"),n("OutboundLink")],1)]),t._v(" "),n("h2",{attrs:{id:"エラー集"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#エラー集"}},[t._v("#")]),t._v(" エラー集")]),t._v(" "),n("h3",{attrs:{id:"エラー1"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#エラー1"}},[t._v("#")]),t._v(" エラー1")]),t._v(" "),n("p",[t._v("学習のためにCityscapesセグメンテーションマスクで処理する必要がありますが，"),n("br"),t._v(" "),n("a",{attrs:{href:"https://github.com/sacmehta/EdgeNets/blob/master/README_Segmentation.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("sacmehta/EdgeNets/README_Segmentation.md"),n("OutboundLink")],1),t._v("に記述されている以下のコマンドをそのまま実行すると，エラーが発生します．"),n("br")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 以下のコードではエラーが発生します")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# ディレクトリの移動")]),t._v("\ncd "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("EdgeNets"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("data_loader"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("cityscape_scripts\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# セグメンテーションマスキング")]),t._v("\npython process_cityscapes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\npython generate_mappings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\nModuleNotFoundError"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" No module named "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utilities'")]),t._v("\n")])])]),n("p",[t._v("これは，pythonのコード中に"),n("code",[t._v("from utilities.print_utils import *")]),t._v("が./EdgeNets/utilitiesをインポートしているので，「process_cityscapes.pyとgenerate_mappings.py」を./EdgeNets直下に置く必要があります．")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("bash\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 現在ディレクトリ：/content/drive/My\\ Drive/segmentation/EdgeNets")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# cp <移動前> <移動先>")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# process_cityscapes.pyを/EdgeNets直下にコピー")]),t._v("\ncp data_loader"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("cityscape_scripts"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("process_cityscapes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py process_cityscapes"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# process_cityscapes.pyを/EdgeNets直下にコピー")]),t._v("\ncp data_loader"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("segmentation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("cityscape_scripts"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("generate_mappings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py generate_mappings"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("py\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);